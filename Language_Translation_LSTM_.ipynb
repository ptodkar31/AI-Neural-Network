{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXXCh4x6nBkb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense,Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 : DataSet Defination\n",
        "\n",
        "This is the dataset where each tuples consist if a simple English phrase and its french translation.This is a small toy dataset fro the purpose of demonstration"
      ],
      "metadata": {
        "id": "AM8QF5FyoKE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    (\"hello\", \"bonjour\"),\n",
        "    (\"how are you\", \"comment ça va\"),\n",
        "    (\"thank you\", \"merci\"),\n",
        "    (\"good morning\", \"bonjour\"),\n",
        "    (\"good night\", \"bonne nuit\"),\n",
        "    (\"see you later\", \"à plus tard\"),\n",
        "    (\"I love you\", \"je t'aime\"),\n",
        "]"
      ],
      "metadata": {
        "id": "jmRNw5CQoJql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Text Preparation\n",
        "\n",
        "zip(*data):Separates the data tuples into two seperate lists:one for input_text(English and onefor target_text(French)"
      ],
      "metadata": {
        "id": "5SgiFerNqNx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts,target_texts = zip(*data)"
      ],
      "metadata": {
        "id": "cBZARs80qklV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4 : Tokenization**\n",
        "\n",
        "**Tokenization():** Creates a tokenizer that will convert text into sequences of integer.\n",
        "\n",
        "**fit_on_texts():** This method creates a vocabulary from the input_texts and target_texts and assigns a unique integer to each word."
      ],
      "metadata": {
        "id": "EusS71qHq1rB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_tokenizer = Tokenizer()\n",
        "target_tokenizer = Tokenizer()\n",
        "\n",
        "input_tokenizer.fit_on_texts(input_texts)\n",
        "target_tokenizer.fit_on_texts(target_texts)"
      ],
      "metadata": {
        "id": "ZU2fwkVMrO9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**texts_to_sequences():** Converts each text(sentences) into a sequence of integers.Each word in the text is replaced by its corresponding integer from the vocabulary"
      ],
      "metadata": {
        "id": "z9Wo3-Q_rije"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequences = input_tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = target_tokenizer.texts_to_sequences(target_texts)"
      ],
      "metadata": {
        "id": "YSBVMQ4XsQc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 5 : Vocabulary and Sequence Length Calculations**\n",
        "\n",
        "**word_index :**\n",
        "This dictionary holds the integer mappings for each word.We add 1 to account for the 0th based indexing of sequences.input_vocab_size and\n",
        "\n",
        "**target_vocab_size:**\n",
        "Stores the size of the vocabulary for the input and target languages"
      ],
      "metadata": {
        "id": "8u8rkVoCsbxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "target_vocab_size = len(target_tokenizer.word_index) + 1"
      ],
      "metadata": {
        "id": "0bE29QqqtAsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**max_input_len and max_target_len :**\n",
        "Store the maximum length of sequences in the input and target languages,respectively.This helps with padding the sequences to a uniform length"
      ],
      "metadata": {
        "id": "xsOlDKDUtNOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_len = max(len(seq) for seq in input_sequences)\n",
        "max_target_len = max(len(seq) for seq in target_sequences)"
      ],
      "metadata": {
        "id": "_oKC1NNZuDkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 6 : Padding Sequences pad_sequences() :**\n",
        "Pads each sequence to ensure that all sequences have the same length.Padding is applied to the end of the sequences(padding=\"post\")"
      ],
      "metadata": {
        "id": "wpGBKCg2uI_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input_data = pad_sequences(input_sequences,maxlen=max_input_len,padding=\"post\")\n",
        "decoder_input_data = pad_sequences(target_sequences,maxlen=max_target_len,padding=\"post\")"
      ],
      "metadata": {
        "id": "zuaghHYnujb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7: One-Hot Encoding Target Sequences\n",
        "\n",
        "np.zeros(): Creates a zero matrix where each row corresponds to a sentence and each column corresponds to a time step in the sequence. The depth corresponds to the size of the vocabulary (for one hot encoding)\n",
        "for loop: Loops over the target sequences and creates one-hot encoded vectors where only the index corresponding to the word is 1. The shift by one ensures that the target data starts predicting from the second word"
      ],
      "metadata": {
        "id": "N2O066ZlB1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_target_data = np.zeros((len(target_sequences),max_target_len,target_vocab_size),dtype=\"float32\")\n",
        "for i, seq in enumerate(target_sequences):\n",
        "    for t,word in enumerate(seq):\n",
        "        if t>0:                    #Target sequence shifted by 1\n",
        "            decoder_target_data[i,t-1,word] = 1.0"
      ],
      "metadata": {
        "id": "verFnx-dCUX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8: Splitting the data\n",
        "\n",
        "train_test_split(): Splits the input data (encoder and decoder inputs) and target data into training and testing sets.\n",
        "\n",
        "test_size=0.2 means 20% of data is used for testing and 80% for training."
      ],
      "metadata": {
        "id": "7pq1ZW78FEaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test, decoder_input_train, decoder_input_test = train_test_split(encoder_input_data, decoder_target_data, decoder_input_data, test_size=0.2)"
      ],
      "metadata": {
        "id": "CkTkbTYeFlq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9: Model Architecture"
      ],
      "metadata": {
        "id": "7lpKuSm9G6TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#embedding_dim = 128 Or any other value you'd like, typically 50, 100, or 300\n",
        "#Define hyperparameters\n",
        "latent_dim = 128          #No. of units in LSTM\n",
        "embedding_dim = 128       #Size of word embeddings"
      ],
      "metadata": {
        "id": "1Jc4AWHsHB0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input(shape=(max_input_len)): Defines the input shape for the encoder (input sentence length).\n",
        "Embedding(): Maps the input word indices to dense vectors of size embedding_dim.\n",
        "LSTM(): The LSTM layer processes the input embeddings and returns two things the final hidden state (state_h) and cell state (state_c). These states will be passed to the decoder."
      ],
      "metadata": {
        "id": "ZFBEa3kPHiiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_inputs = Input(shape=(max_input_len,))\n",
        "encoder_embedding = Embedding(input_vocab_size, embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)"
      ],
      "metadata": {
        "id": "V0c2R6dPIkQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the encoder, the decoder also has an embedding layer followed by an LSTM. The LSTM receives the encoder's final states (state_h, state_c) as initial states for the decoding process.\n",
        "\n",
        "return_sequences = True ensures that the decoder produces a sequence of outputs rather than just the last output."
      ],
      "metadata": {
        "id": "RZ2z4QX_I6w4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_inputs = Input(shape=(max_target_len,))\n",
        "decoder_embedding = Embedding(target_vocab_size, embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])"
      ],
      "metadata": {
        "id": "waNBdggpJIqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dense Layer**\n",
        "\n",
        "Dense(): A fully connected layer that outputs a probability distribution over the target vocabulary (for each word in the sequence).\n",
        "\n",
        "softmax: Ensures the output is a probability distribution."
      ],
      "metadata": {
        "id": "x2mUtaDqLZQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_dense = Dense(target_vocab_size, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "wsIdhWe8Lhit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 10: Defining the Model"
      ],
      "metadata": {
        "id": "efFLt7OlL72L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "-S5sDaVkMAgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Model\n",
        "model.fit([X_train, decoder_input_train], y_train, batch_size=32, epochs=100, validation_data=([X_test, decoder_input_test], y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cq7pD9oZMLb0",
        "outputId": "87e24633-d401-4c64-fd3d-b03c2b373340"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.0667 - loss: 1.0267 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 2/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 0.2667 - loss: 1.0209 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 3/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.2667 - loss: 1.0151 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 4/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3333 - loss: 1.0092 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 5/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3333 - loss: 1.0031 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 6/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 0.9967 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 7/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.3333 - loss: 0.9901 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 8/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3333 - loss: 0.9830 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 9/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.3333 - loss: 0.9755 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 10/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.3333 - loss: 0.9675 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 11/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.3333 - loss: 0.9589 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 12/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.3333 - loss: 0.9495 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 13/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.3333 - loss: 0.9394 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 14/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.3333 - loss: 0.9284 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 15/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step - accuracy: 0.3333 - loss: 0.9164 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 16/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 120ms/step - accuracy: 0.3333 - loss: 0.9033 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 17/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.3333 - loss: 0.8890 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 18/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.3333 - loss: 0.8733 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 19/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.3333 - loss: 0.8561 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 20/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.3333 - loss: 0.8373 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 21/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.2667 - loss: 0.8169 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 22/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2667 - loss: 0.7948 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 23/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2000 - loss: 0.7712 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 24/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.2000 - loss: 0.7467 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 25/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.2000 - loss: 0.7220 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 26/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.2000 - loss: 0.6986 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 27/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2000 - loss: 0.6779 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 28/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.0667 - loss: 0.6612 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 29/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0667 - loss: 0.6494 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 30/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.0667 - loss: 0.6427 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 31/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - accuracy: 0.0667 - loss: 0.6405 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 32/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.0667 - loss: 0.6414 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 33/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.0667 - loss: 0.6442 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 34/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.0667 - loss: 0.6476 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 35/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.0667 - loss: 0.6509 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 36/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.0667 - loss: 0.6540 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 37/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.0667 - loss: 0.6567 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 38/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.1333 - loss: 0.6590 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 39/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1333 - loss: 0.6607 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 40/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 0.1333 - loss: 0.6617 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 41/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.1333 - loss: 0.6615 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 42/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.1333 - loss: 0.6601 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 43/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.1333 - loss: 0.6573 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 44/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.1333 - loss: 0.6531 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 45/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - accuracy: 0.1333 - loss: 0.6480 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 46/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.1333 - loss: 0.6422 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 47/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.1333 - loss: 0.6358 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 48/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.1333 - loss: 0.6289 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 49/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.1333 - loss: 0.6211 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 50/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.2000 - loss: 0.6125 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 51/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - accuracy: 0.2000 - loss: 0.6028 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 52/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2000 - loss: 0.5923 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 53/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.2667 - loss: 0.5813 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 54/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.2667 - loss: 0.5702 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 55/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2667 - loss: 0.5590 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 56/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.2667 - loss: 0.5475 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 57/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.3333 - loss: 0.5356 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 58/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.3333 - loss: 0.5229 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 59/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4000 - loss: 0.5093 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 60/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4000 - loss: 0.4947 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 61/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.4000 - loss: 0.4792 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 62/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.4000 - loss: 0.4629 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 63/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4000 - loss: 0.4461 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 64/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4000 - loss: 0.4294 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 65/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4000 - loss: 0.4132 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 66/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4000 - loss: 0.3978 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 67/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4000 - loss: 0.3833 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 68/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4000 - loss: 0.3696 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 69/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.4000 - loss: 0.3569 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 70/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.4000 - loss: 0.3451 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 71/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.4000 - loss: 0.3340 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 72/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 0.4000 - loss: 0.3239 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 73/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.4000 - loss: 0.3145 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 74/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4000 - loss: 0.3061 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 75/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.4000 - loss: 0.2987 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 76/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step - accuracy: 0.4000 - loss: 0.2922 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 77/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4000 - loss: 0.2866 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 78/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4000 - loss: 0.2819 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 79/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4000 - loss: 0.2776 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 80/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.4000 - loss: 0.2736 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 81/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4000 - loss: 0.2697 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 82/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4000 - loss: 0.2654 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 83/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4000 - loss: 0.2606 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 84/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.4000 - loss: 0.2552 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 85/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.4000 - loss: 0.2492 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 86/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4000 - loss: 0.2432 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 87/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4000 - loss: 0.2385 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 88/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4000 - loss: 0.2352 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 89/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4000 - loss: 0.2329 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 90/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.4000 - loss: 0.2314 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 91/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.4000 - loss: 0.2302 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 92/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4000 - loss: 0.2288 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 93/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - accuracy: 0.4000 - loss: 0.2270 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 94/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - accuracy: 0.4000 - loss: 0.2246 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 95/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4000 - loss: 0.2217 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 96/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4000 - loss: 0.2185 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 97/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4000 - loss: 0.2154 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 98/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4000 - loss: 0.2125 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 99/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 0.4000 - loss: 0.2101 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
            "Epoch 100/100\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.4000 - loss: 0.2083 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f8865ede860>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose of Inference Models\n",
        "\n",
        "#After the model has been trained, we need to define the inference process to actually generate translations.\n",
        "#In the training process, both the encoder and decoder receive complete sequences. However, during inference (prediction), we only have the input sentence,\n",
        "#and the decoder must generate the output word by word, one step at a time.\n",
        "#Thus, we create two separate models for Inference:\n",
        "#Encoder model: Converts the input sentence into internal states (hidden and cell states)\n",
        "#that are passed to the decoder.\n",
        "#Decoder model: Takes the encoder's internal states and generates the output sequence word by word\n",
        "#Define Inference models for translation\n",
        "\n",
        "#Encoder model\n",
        "encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
        "\n",
        "#Purpose: The encoder processes the input sequence and outputs its final internal states\n",
        "#(hidden state state_h and cell state state_c).\n",
        "#These states will be passed to the decoder during inference.\n",
        "#encoder_inputs: The input sequence for the encoder (which is padded).\n",
        "#[state_h, state_c]: The encoder's final states that the decoder will use to start\n",
        "#generating the output sequence.\n",
        "\n",
        "# Decoder model\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "\n",
        "#decoder_state_input_h and decoder_state_input_c: Inputs to the decoder.\n",
        "#These are the hidden state (state_h) and cell state (state_c)\n",
        "#that were produced by the encoder.\n",
        "#In Inference, we don't have these states at the beginning.\n",
        "#so they are taken as Inputs for the decoder.\n",
        "\n",
        "decoder_lstm_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "decoder_model = Model([decoder_inputs, decoder_state_input_h, decoder_state_input_c], [decoder_outputs, decoder_state_h, decoder_state_c])\n",
        "\n",
        "#The decoder LSTH takes in the current word (embedded using the decoder embedding layer)\n",
        "#along with the hidden and cell states (decoder_state_input_h_and_decoder_state_input_c)\n",
        "#as initial states.\n",
        "#decoder_lstm_outputs: The LSTM output for the current time step\n",
        "#(which represents the probabilities for each word in the vocabulary).\n",
        "#decoder_state_h, decoder_state_c: The updated hidden and cell states after\n",
        "#processing the currert word.These states will be passed back into the LSTM for\n",
        "#the next time step.\n",
        "\n",
        "#Function to decode a sequence using the trained model\n",
        "#The fuction takes an Input sequence (from a source language, for example)\n",
        "#and uses an encoder-decoder model to generate a translated sequence ( target language).\n",
        "#It perfores this in an iterative manner, predicting one word at a time,\n",
        "#until it either predicts the end-of-sequence talken or reaches a specified maximum length.\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    #input_seq: This is the sequence that you want to translate.\n",
        "    #The encoder nodel processes the Input sequence and returns the states_value\n",
        "    #(Hidden and cell states) that represent the context learned from the input sequence.\n",
        "    #These states are used as the initial state for the decoder.\n",
        "\n",
        "    target_seq = np.zeros((1, 1))\n",
        "\n",
        "    #target seq: This starts as an array of zeros because at the beginning,\n",
        "    #there is no input to the decoder. As the decoder predicts words,\n",
        "    #this array will hold the index of the word generated at the previous step.\n",
        "    #decoded_sentence: An empty string that will hold the generated translation.\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    #decode_sentence:  An empty string that will hold the generated translation.\n",
        "    #stop_condition:A flag to indicate when the decoding process should stop.\n",
        "\n",
        "\n",
        "    while not stop_condition:\n",
        "    #The loop continues until the translation is complete\n",
        "    #i.e., when the decoder generates an end token or exceeds the allowed length).\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        #decoder model uses the current target sequence (target seq)\n",
        "        #and the encoder's final states (states_value) to predict the next word.\n",
        "        #output tokens: The predicted probabilities of the next word.\n",
        "        #h, c: The updated hidden and cell states. These states are passed to the next iteration to ensure\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        #sampled_token_index: The index of the predicted word.\n",
        "        sampled_char = target_tokenizer.index_word[sampled_token_index]\n",
        "        #sampled_char: The word corresponding to the predicted index.\n",
        "        #decoded_sentence += ' ' + sampled\n",
        "        #This loop continue untill the translation\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        #decoder model uses the current target sequence (target_seq)\n",
        "        #and the encoder's final states (states value) to predict the next word.\n",
        "        #output tokens: The predicted probabilities of the next word.\n",
        "        #h, c: The updated hidden and cell states. These states are passed to the next iteration to ensure\n",
        "\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = target_tokenizer.index_word.get(sampled_token_index, \"\")\n",
        "\n",
        "        #output_tokens [0, 1,:]:\n",
        "        #The output_tokens array contains the predicted probabilities for each possible word\n",
        "        #in the vocabulary.\n",
        "        #The shape of output_tokens is typically (batch_size, sequence_length, vocabulary_size).\n",
        "        #In this case, batch_size is 1 because we are decoding one sentence.\n",
        "        #sequence_length is 1 because at each time step, only one word is generated.\n",
        "        #vocabulary_size is the number of possible words in the target vocabulary.\n",
        "        #output tokens [0, -1,:] selects the predicted probabilities of words at the current time step\n",
        "        #from the vocabulary.\n",
        "        #illustration: Suppose the vocabulary has 5 words: {0: \"hello\", 1: 'world', 2: \"how\", 3: 'are',4:'you}\n",
        "        #The output tokens might look something like this:\n",
        "        #output tokens[0, -1,:]= (0.1, 0.6, 0.05, 0.15, 0.1)\n",
        "        #sampled_token_index=np.argmax(output_tokens[0,-1:]):\n",
        "\n",
        "\n",
        "        #np.argmax() finds the index of the highest probability from the output tokens array.\n",
        "        #In this case, it will select the index 1 because the highest probability (0.6)\n",
        "        #corresponds to the word \"world\".\n",
        "        #Now, using the sampled token_index 1:\n",
        "        #sampled word target tokenizer.index_word.get(1, \"\")\n",
        "        #sampled word \"world\"\n",
        "        #Putting it all together:\n",
        "        #After running np.argmax(), the most likely word's Index (1 in this case) is selected.\n",
        "        #This index is then used to retrieve the corresponding word ('world' in this case)\n",
        "        #from the tokenizer's dictionary.\n",
        "\n",
        "\n",
        "        decoded_sentence += sampled_word + \" \"\n",
        "        # The Predicted word is appended to the decoded_sentence string.\n",
        "        if sampled_word == \"<end>\" or len(decoded_sentence) > max_target_len:\n",
        "            stop_condition = True\n",
        "            # The decoding process stops when the send> token is predicted,\n",
        "            #or if the sentence exceeds the maximum allowed length (max_target_len).\n",
        "            #Update the target sequence for the next iteration\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        # This line creates a 2D NumPy array filled with zeros, with the shape (1, 1).\n",
        "        # In the context of sequence-to-sequence models (such as machine translation),\n",
        "        # this is used to hold the token (word Index) that will be fed as input into the decoder\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        # target seq[0, 0] sampled_token_index:\n",
        "        #This line assigns the value of sampled token index (which is the index of the word predicted\n",
        "        #by the decoder in the previous step) to the target seq. The value is placed at position [0, 0] because it's a 1x1 array, and [0, 0]\n",
        "        #refers to the only element in that array.\n",
        "        # sampled_token Index 1 (from the previous word prediction step).\n",
        "        #After this assignment, the target seq will look like this:\n",
        "        #target seq[0, 0] = 1\n",
        "        #Result: target seq [[1.]]\n",
        "        #Purpose:\n",
        "        #The target seq is used as the input for the decoder at the next tine step.\n",
        "        #At each decoding step, the decoder needs to be fed the token (or word) predicted\n",
        "        #in the previous time step. So, this array is updated with the index of the last\n",
        "        #predicted word (sampled_token_index) and then passed to the decoder for the next prediction.\n",
        "\n",
        "        states_value = [h, c]\n",
        "        #The updated hidden and cell states (h and c) are passed back into the decoder\n",
        "        #to maintain the flow of information across time steps.\n",
        "    return decoded_sentence\n",
        "\n",
        "    # translate(sentence): This function translates a given sentence.\n",
        "    # input_tokenizer.texts_to_sequences([sentence]): Converts the input sentence into a sequence of tokens.\n",
        "    # pad_sequences(): Pads the input sequence to the maximum length (since the model expects inputs to be of uniform length).\n",
        "    # decode_sequence(): Calls the decoding function to generate the translation for the given input sequence.\n",
        "\n",
        "    def translate(sentence):\n",
        "      sequence = input_tokenizer.texts_to_sequences([sentence])\n",
        "      # sentence: This is the input sentence you want to translate (from the source language).\n",
        "      # input_tokenizer.texts_to_sequences([sentence]):\n",
        "      # input_tokenizer is a tokenizer that has already been trained on the source language.\n",
        "      # It contains a vocabulary mapping words to numerical indices (tokens).\n",
        "      # texts_to_sequences converts the sentence (a list of words) into a list of numerical indices\n",
        "      # representing the words in the sentence.\n",
        "      # For example, if the input sentence is \"hello world\" and the tokenizer maps 'hello' to 1\n",
        "      # and 'world' to 2, the resulting sequence will be [1, 2].\n",
        "      sequence = pad_sequences(sequence, maxlen=max_input_len, padding='post')\n",
        "      # pad_sequences():\n",
        "      # This function ensures that all sequences (inputs) are of the same length.\n",
        "      # Since neural networks often require fixed-length input, the input sequence is either\n",
        "      # truncated (if too long) or padded with zeros (if too short) to match the required length.\n",
        "      # maxlen=max_input_len: The maximum length that the input sequence should be.\n",
        "      # This is a predefined length based on how the model was trained.\n",
        "      # padding=\"post\": If padding is needed, zeros will be added to the end (or \"post\") of the sequence.\n",
        "\n",
        "      # Result: sequence = [[1, 2, 0, 0, 0]] (example)\n",
        "      translation = decode_sequence(sequence)\n",
        "      # This function is the core of the translation process. It takes the processed input sequence (now padded).\n",
        "      # Inside the decode_sequence function, the model predicts one word at a time\n",
        "      # (as explained earlier) until it reaches an end token (<end>) or a maximum sentence length.\n",
        "\n",
        "      return translation\n",
        "\n",
        "\n",
        "#Example Usage\n",
        "translated_sentence = translate(\"hello\")\n",
        "print(\"Translated sentence:\", translated_sentence)"
      ],
      "metadata": {
        "id": "L3zkV3kCTQzV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b61737-551c-49db-944a-a6d1d22d9e50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f883ffd60e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 7 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f883ffd65f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Translated sentence: nuit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Purpose of Inference models\n",
        "# After the model has been trained, we need to define the inference process to actually generate translation\n",
        "# In this training process, both the encoder and decoder receive complete sequences.\n",
        "# However, during inference (prediction), we only have the input sentence, and the decoder must generate the output word by word, one step at a time.\n",
        "# Thus, we create two seperate models for inference:\n",
        "\n",
        "# Encoder model: Converts the input sentence into internal states (hidden and cell states)\n",
        "# that are passed to the decoder.\n",
        "# Decoder model: Takes the encoder's internal states and generates the output sequence word by word.\n",
        "\n",
        "# Define inference models for translation\n",
        "\n",
        "# Encoder model\n",
        "encoder_model = Model(encoder_inputs, [state_h, state_c])\n",
        "\n",
        "# Purpose: The encoder processes the input sequence and outputs its final internal states\n",
        "# (hidden state state_h and cell state state_c).\n",
        "# These states will be passed to the decoder during inference.\n",
        "# encoder_inputs: The input sequence for the encoder (which is padded).\n",
        "# [state_h, state_c]: The encoder's final states that the decoder will use to start\n",
        "# generating the output sequence.\n",
        "\n",
        "# Decoder model\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "# decoder_state_input_h and decoder_state_input_c: Inputs to the decoder.\n",
        "# These are the hidden state (state_h) and cell state (state_c)\n",
        "# that were produced by the encoder.\n",
        "# In inference, we don't have these states at the beginning,\n",
        "# so they are taken as inputs for the decoder.\n",
        "\n",
        "decoder_lstm_outputs, decoder_state_h, decoder_state_c = decoder_lstm(\n",
        "    decoder_embedding, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        ")\n",
        "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs, decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs, decoder_state_h, decoder_state_c],\n",
        "\n",
        ")\n",
        "# The decoder LSTM takes in the current word(embedded using the decoder_embedding_layer)\n",
        "# along with the hidden and cell states (decoder_state_input_h and decoder_state_input_c)\n",
        "# as initial states.\n",
        "# decoder_lstm_outputs: The LSTM output for the current for the time stop\n",
        "# (which representa the probabilities for each word in the vocabulary).\n",
        "# decoder_state_h, decoder_state_c: The updated and cell states after preprocessing the current word.\n",
        "# These states will be passed back into the LSTM for the next step.\n",
        "# decoder_outputs: The output probabilities for each word in the vocabulary.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Function to decode a sequence using the trained model\n",
        "# The function takes an input sequence (from a source language, for example)\n",
        "# and uses an encoder-decoder model to generate a translated sequence (target language).\n",
        "# It performs this in an iterative manner, predicting one word at a time,\n",
        "# until it either predicts the end-of-sequence token or reaches a specified maximum length.\n",
        "def decode_sequence(input_seq):\n",
        "     # Get the states from the encoder model\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "# input_seq: This is the sequence that you want to translate.\n",
        "# The encoder_model processes the input sequence and returns the states_value\n",
        "# (hidden and cell states) that represent the context learned from the input sequence.\n",
        "# These states are used as the initial state for the decoder\n",
        "\n",
        "    target_seq = np.zeros((1,1))\n",
        "# target_seq: This starts as an array of zeros because at the beginning.\n",
        "# there is no input to the decoder. As the decoder predicts words,\n",
        "# This array will hold the index of the word generated at the previous step.\n",
        "\n",
        "\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "# decoded_sentence: An empty string will hold the generated translation.\n",
        "# stop_condition: A flag to indicate when the decoding process should stop.\n",
        "    while not stop_condition:\n",
        "# The loop continues until the translation is complete\n",
        "# (i.e., when the decoder generates an end token or exceeds the allowed length).\n",
        "      output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "# decoder_model uses the current target sequence (target_seq)\n",
        "# and the encoder's final states (states_value) to predict the next word.\n",
        "# output_tokens: The predicted probabilities of the next word.\n",
        "# h, c: The updated hidden and cell states. These states are passed to the next iteration to ensure continuity.\n",
        "\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "      sampled_word = target_tokenizer.index_word.get(sampled_token_index, \"\")\n",
        "# output_tokens[0, -1, :]\n",
        "\n",
        "#The output_tokens array contains the predicted probabilities for each possible word in the vocal\n",
        "#The shape of output_tokens is typically (batch_size, sequence_length, vocabulary_size).\n",
        "#In this case, batch_size is 1 because we are decoding one sentence.\n",
        "\n",
        "\n",
        "#sequence_length is 1 because at each time step, only one word is generated.\n",
        "#vocabulary_size is the number of possible words in the target vocabulary.\n",
        "#output tokens [0, 1] selects the predicted probabilities of words at the current time step from the vocabulary.\n",
        "# Illustration: Suppose teh vocabulary has 5 words:  (0: 'hello', 1: 'world', 2: 'how', 3: 'are', 4: 'you')\n",
        "#The output_tokens might look something like this:\n",
        "#output_tokens [0, -1, :] = [0.1, 0.6, 0.05, 0.15, 0.1]\n",
        "#sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "\n",
        "#np.argmax() finds the index of the highest probability from the output tokens array.\n",
        "#In this case, it will select the index 1 because the highest probability (0.6)\n",
        "#corresponds to the word 'world'.\n",
        "#Now, using the sampled_token_index = 1:\n",
        "#sampled_word=target_tokenizer.index_word.get(1, \"\")\n",
        "#sampled_word = \"world\"\n",
        "#Putting it all together:\n",
        "#After running np.argmax(), the most likely word's index (1 in this case) is selected.\n",
        "#This index is then used to retrieve the corresponding word ('world' in this case)\n",
        "#from the tokenizer's dictionary.\n",
        "      decoded_sentence += sampled_word + \" \"\n",
        "      # The predicted word is appended to the decoded_sentence stirng.\n",
        "\n",
        "      if sampled_word == \"<end>\" or len(decoded_sentence) > max_target_len:\n",
        "        stop_condition = True\n",
        "\n",
        "  # The decoding process stops when the <end> token is predicted .\n",
        "  # or if the sentence exceeds the maximum allowed length (max_target_len).\n",
        "# Update the target sequence for the next iteration:\n",
        "      target_seq = np.zeros((1,1))\n",
        "\n",
        "  # This line creates a 2D numpy array filled with zeros, with the shape (1,1).\n",
        "# In the context of sequence-to-sequence models (such as mahcine translation)\n",
        "# this is used to hold the token(word index) that will be fed as input into the decoder\n",
        "# at the time step\n",
        "      target_seq[0,0] = sampled_token_index\n",
        "# target_seq[0,0] = sampled_token_index:\n",
        "# This line assign the value of sampled_token_index (which is the index of the word predicted\n",
        "# by the decoder inthe previous step) to the target_seq\n",
        "# The value is palced at position [0,0] because it's a 1*1 array, and [0,0]\n",
        "# refers to the only element in the array.\n",
        "\n",
        "#sampled_token_index= 1 (from the previous word prediction step).\n",
        "#After this assignment, the target_seq will look like this:\n",
        "#target_seq[0, 0] = 1\n",
        "#Result: target_seq= [[1.]]\n",
        "#Purpose:\n",
        "#The target_seq is used as the input for the decoder at the next time step.\n",
        "#At each decoding step, the decoder needs to be fed the token (or word) predicted\n",
        "#in the previous time step. So, this array is updated with the index of the last\n",
        "# predicted work (sampled_token_index) and then passed to the decoder for the next prediction.\n",
        "      states_value = [h,c]\n",
        "# The updated hidden and cell states (h and c ) are passed back into the decoder\n",
        "# to maintain the flow of information across time steps:\n",
        "    return decoded_sentence\n",
        "\n",
        "# translate(sentence): This function translates a given sentence.\n",
        "\n",
        "# input_tokenizer.texts_to_sequences([sentence]): Converts the input sentence into a sequence of tokens.\n",
        "# pad_sequences(): Pads the input sequence to the maximum length (since the model expects inputs to be of uniform length).\n",
        "# decode_sequence(): Calls the decoding function to generate the translation for the given input sequence.\n",
        "\n",
        "# Translate a sentence\n",
        "def translate(sentence):\n",
        "\n",
        "    sequence = input_tokenizer.texts_to_sequences([sentence])\n",
        "\n",
        "    # sentence: This is the input sentence you want to translate (from the source language).\n",
        "    # input_tokenizer.texts_to_sequences([sentence]):\n",
        "    # input_tokenizer is a tokenizer that has already been trained on the source language.\n",
        "    # It contains a vocabulary mapping words to numerical indices (tokens).\n",
        "    # texts_to_sequences converts the sentence (a list of words) into a list of numerical indices\n",
        "    # representing the words in the sentence.\n",
        "    # For example, if the input sentence is \"hello world\" and the tokenizer maps 'hello' to 1\n",
        "    # and 'world' to 2, the resulting sequence will be [1, 2].\n",
        "\n",
        "    sequence = pad_sequences(sequence, maxlen=max_input_len, padding=\"post\")\n",
        "\n",
        "    # pad_sequences():\n",
        "    # This function ensures that all sequences (inputs) are of the same length.\n",
        "    # Since neural networks often require fixed-length input, the input sequence is either\n",
        "    # truncated (if too long) or padded with zeros (if too short) to match the required length.\n",
        "    # maxlen=max_input_len: The maximum length that the input sequence should be.\n",
        "    # This is a predefined length based on how the model was trained.\n",
        "    # padding=\"post\": If padding is needed, zeros will be added to the end (or \"post\") of the sequence.\n",
        "\n",
        "    # Result: sequence = [[1, 2, 0, 0, 0]] (example)\n",
        "\n",
        "    translation = decode_sequence(sequence)\n",
        "\n",
        "    # This function is the core of the translation process. It takes the processed input sequence (now padded).\n",
        "    # Inside the decode_sequence function, the model predicts one word at a time\n",
        "    # (as explained earlier) until it reaches an end token (<end>) or a maximum sentence length.\n",
        "\n",
        "    return translation\n",
        "\n",
        "# Example usage:\n",
        "translated_sentence = translate(\"hello world\")\n",
        "print(\"Translated sentence:\", translated_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxjZjUr1GocO",
        "outputId": "a7a7e344-7fa5-47c7-83b9-ab11bfd47273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 305ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\n",
            "Translated sentence: nuit \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUFwZ0wOIS_b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}