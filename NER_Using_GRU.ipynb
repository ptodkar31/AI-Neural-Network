{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense,TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "jmZvSYSEqDCW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AdzfKDBAjEQr"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Sample training sentence (tokenized manually)\n",
        "#These are example sentences for tarining\n",
        "#Each sentence is tokenized (split into words)\n",
        "train_sentences=[\n",
        "    [\"john\",\"lives\",\"in\",\"New\",\"York\"],\n",
        "    [\"Alice\",\"is\",\"from\",\"Paris\"],\n",
        "    [\"Berlin\",\"is\",\"the\",\"capital\",\"of\",\"Germany\"]\n",
        "]\n",
        "\n",
        "#Corresponding NER tags for training data'\n",
        "#Corresponding NER (Named Entity Recognition) tags for each word.\n",
        "#B-PER : Beginning of a person entity\n",
        "#B-Loc: Beginning of a location entity\n",
        "#O : Outside, meaning no entity\n",
        "\n",
        "train_ner_tags = [\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"B-LOC\"],\n",
        "    [\"B-PER\", \"O\", \"O\", \"B-LOC\"],\n",
        "    [\"B-LOC\", \"O\", \"O\", \"O\", \"O\",\"B-LOC\"]\n",
        "]\n",
        "\n",
        "\n",
        "#Vocabulary and tag mappings(for tokenization)\n",
        "\n",
        "vocab={\n",
        "    \"john\":1,\"lives\":2,\"in\":3,\"New\":4,\"York\":5,\"Alice\":6,\"is\":7,\n",
        "    \"from\":8,\"Paris\":9,\"Berlin\":10,\"the\":11,\"capital\":12,\"of\":13,\"Germany\":14\n",
        "}\n",
        "tags={\"O\":0,\"B-LOC\":1,\"B-PER\":2}\n",
        "#The vocab dictionary maps eaxh word in the training sentences to a unique integer.\n",
        "#This is needed because the neural network dosen't operate on words but on umeric values\n",
        "#The tags dictionary maps each NER tag to the Integer (0 for o,1 for B-LOC,2 for B-PER).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the sentences and labels\n",
        "tokenized_train_sentences=[[vocab[word] for word in sentence] for sentence in train_sentences]\n",
        "tokenized_train_ner_tags=[[tags[tag] for tag in ner] for ner in train_ner_tags]\n",
        "#This converts the Training sentences and NER tags into lists of intergers, using the mapping from vocab and tags\n",
        "#Sentences:[\"john\",\"lives\",\"in\",\"New\",\"York\"] because [1,2,3,4,5]."
      ],
      "metadata": {
        "id": "-R0MZM3IpZ5a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extend vocabulary to include new test words\n",
        "vocab.update({\n",
        "    \"Mary\":15,\"visited\":16,\"London\":17,\"Tom\":18,\"moved\":19,\"to\":20,\"statue\":21\n",
        "})\n",
        "\n",
        "#Here, we  extend the vocabulary to inclued additional words from the test sentence.\n",
        "# for example,\" mary\" is mapped to 15, \"london\" to 17 etc."
      ],
      "metadata": {
        "id": "jrUCiJnKp1Wk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the test sentences\n",
        "test_sentences=[\n",
        "    [\"Marry\",\"visisted\",\"London\"],\n",
        "    [\"Tom\",\"moved\",\"to\",\"Berlin\"],\n",
        "    [\"The\",\"statue\",\"is\",\"in\",\"Paris\"]\n",
        "]\n",
        "tokenized_train_sentences=[[vocab.get(word,0) for word in sentence] for sentence in test_sentences]\n",
        "#The 0 in vocab.get(word, 0) serves as the default value returned\n",
        "#if a word is not found in the vocab dictionary. Here's how it works:\n",
        "#vocab.get(word, 0) looks up word in the vocab dictionary.\n",
        "#If the word exists in vocab, it returns its corresponding value (likely a token or index).\n",
        "#If the word does not exist in vocab, it returns e as a fallback."
      ],
      "metadata": {
        "id": "Uxa0cTtKrmN8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "vocab_size=len(vocab)+1 # Updated vocab size to account for all words in the vocab\n",
        "embedding_dim=64 #Dimension of embedding vectors\n",
        "n_tags=len(tags) #Number of entity tags\n",
        "max_len=6 #Max sentence length (after padding)\n",
        "\n",
        "# vocab size: Total number of unique words in the vocabulary (plus 1 for padding).\n",
        "# embedding din: The size of word embeddings (vectors representing each word).\n",
        "# n_tags: The number of MER tags (O, B-LOC, B-PER).\n",
        "# max_len: Maximun sentence length for padding.\n",
        "# Padding training and test sequences\n",
        "\n",
        "tokenized_train_sentences=pad_sequences(tokenized_train_sentences,maxlen=max_len,padding=\"post\")\n",
        "tokenized_train_ner_tags=pad_sequences(tokenized_train_ner_tags,maxlen=max_len,padding=\"post\")\n",
        "tokenized_test_sentences=pad_sequences(tokenized_train_sentences,maxlen=max_len,padding=\"post\")"
      ],
      "metadata": {
        "id": "s0SZO5KDr5_c"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the Training data (Train/Test split)\n",
        "X_train,X_test,y_train,y_test=train_test_split(tokenized_train_sentences,tokenized_train_ner_tags,test_size=0.2)"
      ],
      "metadata": {
        "id": "hx4dx3Gkr6Ls"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Creation\n",
        "model=Sequential()"
      ],
      "metadata": {
        "id": "E1h7-RrNu7yt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embedding layer adjusted to the current vocab_size\n",
        "model.add(Embedding(input_dim=vocab_size,output_dim=embedding_dim,input_length=max_len))"
      ],
      "metadata": {
        "id": "b4QZLWimu71V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "512d8674-666d-4447-eda4-986e935d5645"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(Bidirectional(GRU(units=embedding_dim,return_sequences=True)))  #Bidirectional GRU\n",
        "model.add(TimeDistributed(Dense(n_tags, activation=\"softmax\")))\n",
        "#Optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "Dg_4d4VPu749"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile Model\n",
        "model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "cREMqfVMuMgo"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "history=model.fit(X_train,y_train,batch_size=32,epochs=10,validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEGG3QfmuMjS",
        "outputId": "f8526daa-64e1-4998-b5e1-9fae5186ab8c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8s/step - accuracy: 0.3333 - loss: 1.0931 - val_accuracy: 0.1667 - val_loss: 1.0964\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - accuracy: 0.8333 - loss: 1.0829 - val_accuracy: 0.6667 - val_loss: 1.0874\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.5000 - loss: 1.0729 - val_accuracy: 0.6667 - val_loss: 1.0785\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step - accuracy: 0.5000 - loss: 1.0630 - val_accuracy: 0.6667 - val_loss: 1.0695\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step - accuracy: 0.5000 - loss: 1.0531 - val_accuracy: 0.6667 - val_loss: 1.0605\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.5000 - loss: 1.0432 - val_accuracy: 0.6667 - val_loss: 1.0513\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.5000 - loss: 1.0333 - val_accuracy: 0.6667 - val_loss: 1.0421\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5000 - loss: 1.0232 - val_accuracy: 0.6667 - val_loss: 1.0327\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.5000 - loss: 1.0131 - val_accuracy: 0.6667 - val_loss: 1.0231\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.5000 - loss: 1.0028 - val_accuracy: 0.6667 - val_loss: 1.0134\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHXkNii1uMmo",
        "outputId": "f9b54508-b66c-45f7-c621-04af4114174a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.6667 - loss: 0.9749\n",
            "Test Loss: 0.9748923778533936, Test Accuracy: 0.6666666865348816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on new test sentences\n",
        "predictions=model.predict(tokenized_test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKS4KsxivM3M",
        "outputId": "8e41dbf8-0782-48cc-e8ed-46c8e23f4e52"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 205ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding the Predictions back to NER tags\n",
        "reverse_tags={i:tag for tag,i in tags.items()}\n",
        "\n",
        "'''\n",
        "purpose of reverse_tags:\n",
        "Original tags dictionary: This typically maps tags(e.g, labels, classes)\n",
        "to unique numerical IDs or indices.\n",
        "Example: tags = {\"NOUN\":1,\"VERB\":2,\"ADJECTIVE\":3}\n",
        "Reversed reverese_tags dictionary: The reverse dictionary swaps the roles,\n",
        "mapping the numerical ID's for indices back to their corresponding tags.\n",
        "Example: reverse_tags={1:\"NOUN\",2:\"VERB\",3:\"ADJECTIVE\"}\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "rHbm-qmZvM5m",
        "outputId": "1143402f-a7d6-404b-cff0-a0d7d6efa2b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npurpose of reverse_tags:\\nOriginal tags dictionary: This typically maps tags(e.g, labels, classes)\\nto unique numerical IDs or indices.\\nExample: tags = {\"NOUN\":1,\"VERB\":2,\"ADJECTIVE\":3}\\nReversed reverese_tags dictionary: The reverse dictionary swaps the roles,\\nmapping the numerical ID\\'s for indices back to their corresponding tags.\\nExample: reverse_tags={1:\"NOUN\",2:\"VERB\",3:\"ADJECTIVE\"}\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_predictions(preds, max_len):\n",
        "    decoded_predictions=[]\n",
        "    #This list will store the decoded predictions for all sentences.\n",
        "    #Iterating over each prediction (pred) for every token in each sentence\n",
        "\n",
        "    for pred in preds:\n",
        "        decoded_Sentence=[reverse_tags[np.argmax(p)] for p in pred]\n",
        "        #pred represents the list of predicted probabilities for a token.\n",
        "#For each token p, we apply np.argmax(p) to get the index of the highest probability,\n",
        "#This tells us which NEll tag has the highest probability.\n",
        "#For example, if the probabilities for a token are [8.1, 0.8, 0.1]\n",
        "#then np.argmax(p) returns 1 (because 6.8 is the highest value),\n",
        "#which corresponds to the tag B-LOC (fros reverse_tags).\n",
        "#We use reverse_tags[np.argmax(p) to look up the actual tag (like \"0\", \"B-LOC\", or \"B-PER\")\n",
        "# based on the index returned by np.argmax(p)\n",
        "        decoded_predictions.append(decoded_Sentence)\n",
        "    return decoded_predictions\n",
        "#Suppose the preds for one sentence look like this (simplified to one token per sentence for clar\n",
        "#prods - [\n",
        "#[10.2, 0.7, 8.1], [8.9, 0.05, 0.05], [0.1, 0.2, 0.7]] #Probabilities for three tokens\n",
        "# ]\n",
        "#The model output probabilities for three tokens (one for each possible NER tag):\n",
        "#For token 1: [0.2, 0.7, 0.1] highest probability is at index 1 -> B-LOC.\n",
        "#For token 2: 10.9, 0.05, 0.05] highest probability is at index 0 ->  O.\n",
        "#For token 3: [0.1, 0.2, 0.7] highest probability is at Index 2 -> B-PER.\n",
        "#After decoding, you get\n",
        "#Decoded_sentence =['B-LOC','O','B-PER']"
      ],
      "metadata": {
        "id": "L1_IKeVlvM7o"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoding prediction\n",
        "decode_predictions=decode_predictions(predictions,max_len)"
      ],
      "metadata": {
        "id": "IlKxwY9BvM93"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show test sentences with predicted tags\n",
        "for sentence, pred_tags in zip(test_sentences, decode_predictions):\n",
        "    print(f\"Sentence: {' '.join(sentence)}\")\n",
        "    print(f\"Predicted NER Tags: {pred_tags}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBwgb4lEvNBZ",
        "outputId": "02b303ca-67a7-45c1-e927-871bbfbb6a65"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Marry visisted London\n",
            "Predicted NER Tags: ['O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Sentence: Tom moved to Berlin\n",
            "Predicted NER Tags: ['O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Sentence: The statue is in Paris\n",
            "Predicted NER Tags: ['O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}